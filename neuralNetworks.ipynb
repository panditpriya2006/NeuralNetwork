{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOrTogJYTueymcA/ETVNlL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/panditpriya2006/NeuralNetwork/blob/main/neuralNetworks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zG0vM7SRoL1L",
        "outputId": "949fedba-9251-4d50-b274-fa547d3ebcc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output After Training:\n",
            "[[0.00966449]\n",
            " [0.00786506]\n",
            " [0.99358898]\n",
            " [0.99211957]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np #linear alg library\n",
        "\n",
        "# sigmoid function -> maps any value to a value between 0 and 1... use it to convert numbers to probabilities\n",
        "def nonlin(x,deriv=False):\n",
        "    if(deriv==True): #function can generate derivative of a sigmoid here\n",
        "        return x*(1-x) #derivative\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "# input dataset\n",
        "X = np.array([  [0,0,1],\n",
        "                [0,1,1],\n",
        "                [1,0,1],\n",
        "                [1,1,1] ]) #initializes our input dataset-> each row is a single training example\n",
        "                #each column corresponds to one of our input nodes -> 3 input nodes to the network and 4 training examples\n",
        "\n",
        "# output dataset\n",
        "y = np.array([[0,0,1,1]]).T #output dataset, single row and four columns \"T\" is the transpose func-> after y martix has 4 rows and one column\n",
        "#each row is a training example and each column is an output mode, 3 inputs-> one output (ex. 001->0)\n",
        "\n",
        "# seed random numbers to make calculation\n",
        "# deterministic (just a good practice)\n",
        "np.random.seed(1) #seed your random numbers\n",
        "\n",
        "# initialize weights randomly with mean\n",
        "syn0 = 2*np.random.random((3,1)) - 1 #first layer of weights between l0 and l1\n",
        "#weight matrix-> only have input and output layer so only need one martix of weights to connect them\n",
        "#dimension is (3,1) because 3 inputs and 1 output l0 is size 3 l1 is 1 want to connect every node in l0 to every node in l1\n",
        "for iter in range(10000): #iterates multiple time over training code to optimize our network\n",
        "# forward propagation\n",
        "    l0 = X #first layer of the network (just our data)\n",
        "    l1 = nonlin(np.dot(l0,syn0)) #second layer of the network(hidden) ->prediction step-> let the network try to predict the ouput\n",
        "\n",
        "    # how much did we miss?\n",
        "    l1_error = y - l1 # compare how well it did\n",
        "\n",
        "    # multiply how much we missed by the\n",
        "    # slope of the sigmoid at the values in l1\n",
        "    l1_delta = l1_error * nonlin(l1,True) # -> the error weighted derivative\n",
        "#small error and small slope means very small update\n",
        "    # update weights\n",
        "    syn0 += np.dot(l0.T,l1_delta) #updates weights\n",
        "\n",
        "print (\"Output After Training:\")\n",
        "print (l1)"
      ]
    }
  ]
}